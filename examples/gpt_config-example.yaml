# What sampling temperature to use, between 0 and 2.
# Higher values like 0.8 will make the output more random,
# while lower values like 0.2 will make it more focused and deterministic.
temperature: 0.5

# An alternative to sampling with temperature, called nucleus sampling,
# where the model considers the results of the tokens with top_p probability mass.
# So 0.1 means only the tokens comprising the top 10% probability mass are considered.
top_p: 1

# The prompt being feed into ChatGPT before the text to translate.
# Use {to_lang} to indicate where the target language name should be inserted.
# Tokens used in this example: 57+
chat_system_template: >
  You are an untrammelled professional translation engine.
  Please translate the text into colloquial, elegant and fluent {to_lang},
  without referencing machine translations.
  You must only translate the story, never interpret it.
  If already in {to_lang} or looks like gibberish, output it as is.

  Translate to {to_lang}.


chatgpt:  
  # Should the `Prompt Template` (defined below) text be prepended to the translation requests?
  include_template: True

  # Asking ChatGPT to output as JSON seems to reduce translation refusal rates.
  #   Enable if encountering frequent translation refusals.
  json_mode: True

  # Overwrite configs for a specific model.
  gpt-4o-mini:
    temperature: 0.4
  gpt-3.5-turbo:
    temperature: 0.3

# The text to prepend to `User` messages to GPT before the text to translate.
# Use {to_lang} to indicate where the target language name should be inserted.
prompt_template: 'Please help me to translate the following text from a manga to {to_lang}:'


#  If it's already in {to_lang} or looks like gibberish you have to output it as it is instead.
#  Keep prefix format.


# Samples being feed into ChatGPT to show an example conversation.
# In a [prompt, response] format, keyed by the target language name.
#
# Generally, samples should include some examples of translation preferences, and ideally
# some names of characters it's likely to encounter.
#
# If you'd like to disable this feature, just set this to an empty list.
chat_sample:
  Simplified Chinese: # Tokens used in this example: 88 + 84
    - <|1|>恥ずかしい… 目立ちたくない… 私が消えたい…
      <|2|>きみ… 大丈夫⁉
      <|3|>なんだこいつ 空気読めて ないのか…？
    - <|1|>好尴尬…我不想引人注目…我想消失…
      <|2|>你…没事吧⁉
      <|3|>这家伙怎么看不懂气氛的…？
  English: 
    - <|1|>恥ずかしい… 目立ちたくない… 私が消えたい…
      <|2|>きみ… 大丈夫⁉
      <|3|>なんだこいつ 空気読めて ないのか…？
    - <|1|>I'm embarrassed... I don't want to stand out... I want to disappear...
      <|2|>Are you okay?!
      <|3|>What the hell is this person? Can't they read the room...?

# Sample input & output for when using `json_mode: True`.
# In a [prompt, response] format, keyed by the target language name.
#
# Generally, samples should include some examples of translation preferences, and ideally
# some names of characters it's likely to encounter.
# 
# NOTE: If no JSON sample for the target language is provided, 
#       it will look for a sample from the `chat_sample` section and convert it to JSON if found.
json_sample:
  Simplified Chinese:
    - TextList:  &JSON-Sample-In
        - ID: 1
          text: "恥ずかしい… 目立ちたくない… 私が消えたい…"
        - ID: 2
          text: "きみ… 大丈夫⁉"
        - ID: 3
          text: "なんだこいつ 空気読めて ないのか…？"
    - TextList:
        - ID: 1
          text: "好尴尬…我不想引人注目…我想消失…"
        - ID: 2
          text: "你…没事吧⁉"
        - ID: 3
          text: "这家伙怎么看不懂气氛的…？"
  English: 
    - TextList: *JSON-Sample-In
    - TextList:
        - ID: 1
          text: "I'm embarrassed... I don't want to stand out... I want to disappear..."
        - ID: 2
          text: "Are you okay?!"
        - ID: 3
          text: "What the hell is this person? Can't they read the room...?"

